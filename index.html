<!doctype html>
<html lang="en">

	<head>
		<meta charset="utf-8">

		<title>Apache Spark - Josh Fermin</title>

		<meta name="description" content="Apache Spark - Cluster Computing">
		<meta name="author" content="Josh Fermin">

		<meta name="apple-mobile-web-app-capable" content="yes" />
		<meta name="apple-mobile-web-app-status-bar-style" content="black-translucent" />

		<meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=no, minimal-ui">

		<link rel="stylesheet" href="css/reveal.css">
		<link rel="stylesheet" href="css/theme/sky.css" id="theme">

		<!-- Code syntax highlighting -->
		<link rel="stylesheet" href="lib/css/zenburn.css">

		<!-- Printing and PDF exports -->
		<script>
			var link = document.createElement( 'link' );
			link.rel = 'stylesheet';
			link.type = 'text/css';
			link.href = window.location.search.match( /print-pdf/gi ) ? 'css/print/pdf.css' : 'css/print/paper.css';
			document.getElementsByTagName( 'head' )[0].appendChild( link );
		</script>

		<!--[if lt IE 9]>
		<script src="lib/js/html5shiv.js"></script>
		<![endif]-->
	</head>

	<body>

		<div class="reveal">

			<!-- Any section element inside of this container is displayed as a slide -->
			<div class="slides">
				<section>
					<h1>Apache Spark</h1>
					<p>
						<small>By <a href="http://joshfermin.me">Josh Fermin</a></small>
					</p>
				</section>

				<section>
					<h2>What is Apache Spark?</h2>
					<p class="fragment">
						Open source data analytics cluster computing framework
					</p>
					<p class="fragment">
						Real-Time vs Interactive vs Batch processing
					</p>
					<p class="fragment">
						Write apps in Java, Scala or Python.
					</p>
					<p class="fragment">
						Map Reduce is only a part of supported capabilities.
					</p>

					<aside class="notes">
						Versatile data-processing framework â€“ can run both in memory and on disk

						Allows you to do a variety of things including real-time vs interactive vs batch

						can write applications in Java, Scala, Python.
					</aside>
				</section>

				<!-- Example of nested vertical slides -->
				<section>
					<section>
						<h2>History</h2>
						<img src="lib/img/timeline.png"></img>
						<aside class="notes">
							As you can see, map reduce emerged around 2002 and hadoop following around 2006 and then spark came around 2010, which came from the limitations of map reduce
						</aside>
					</section>
					<section>
						<h2>History - Map Reduce</h2>
						<p>
							Two major problems:
						</p>
						<ul>
							<li class="fragment">difficulty programming in Map Reduce</li>
							<li class="fragment">Batch style jobs not fitting all use cases</li>
						</ul>
					</section>
					<section>
						<h2>History - Workarounds</h2>
						<img src="lib/img/workarounds.png"></img>

						<aside class="notes">
							many different technologies came out because of the limitations of map reduce. Storm being a real time processing framework. Pregel - large scale graph computing
						</aside>
					</section>
				</section>

				<section>
					<section>
						<h2>Spark - Goals</h2>
						<p class="fragment">Generalize Map Reduce</p>
						<ul>
							<li class="fragment">fast data sharing</li>
							<li class="fragment">general DAGs (Directed Acyclic Graphs)</li>
						</ul>
						<aside class="notes">
							goal was to generalize map reduce to support new apps with the same engine - SPARK. Two small additions are enough to express the previous models.
							approach that is more efficient for engine and much simpler for end users
						</aside>
					</section>
					<section>
						<img src="lib/img/codesize.png"></img>
					</section>
				</section>

				<section>
					<section>
						<h2>Learning Spark API</h2>
						<p>
						<a href="https://spark.apache.org/docs/latest/quick-start.html">Problem:</a> Count how many lines a word shows up in a document
						</p>
						<aside class="notes">
							
						</aside>
					</section>
					<section>
						<h2>Spark Shell - Scala</h2>
						<small>Spark Download <a href="https://spark.apache.org/downloads.html">here</a> (use prebuilt version)</small>
							<pre><code>./bin/spark-shell/</pre></code>
							<pre><code>./bin/pyspark/</pre></code>
							
						<aside class="notes">
							Starts the shell in scala using ./bin/spark-shell/
							can also start in python ./bin/pyspark

							can download spark there, need to build or have prebuilt version for these commands to work
						</aside>
					</section>

					<section>
					<h2> Spark Context </h2>
					<pre><code>scala> val textFile = sc.textFile("README.md")
textFile: spark.RDD[String] = spark.MappedRDD@2ee9b6e3</pre></code>
					<ul>
						<li>Start spark context</li>
						<li>Creates RDD (Resilient Distributed Dataset)</li>
					</ul>
					<aside class="notes">
						Next line starts a spark context which represents a connection to a spark cluster.
						Can be used to create RDDs.
					</aside>
					</section>
					
					<section>
						<h2>RDDs</h2>
						<ul>
							<li>Spark's primary abstraction</li>
							<li>Distributed collection of items</li>
							<li>RDDs can be created from Hadoop input formats (HDFS)</li>
						</ul>
					</section>
					<section data-markdown>
					<script type="text/template">
					<h2>RDD Actions</h2>
					```scala
					scala> textFile.count() // Number of items in this RDD
					res0: Long = 126

					scala> textFile.first() // First item in this RDD
					res1: String = # Apache Spark
					```

					Note: RDDs have actions which return values. 
					</script>
					</section>

					<section>
					<h2>RDD Transformations</h2>
					<pre><code>
val linesWithSpark = textFile.filter(line => line.contains("Spark"))
linesWithSpark: spark.RDD[String] = spark.FilteredRDD@7dd4af09
					</code></pre>
					<aside class="notes">
					RDDs also have transformations which returns a pointer to a new RDDs, in this case it will return a subset of the previous RDD with the lines that only contain the word "Spark"
					</aside>
					</section>

					<section data-markdown>
					<script type="text/template">
						<h2>Chaining Actions and transformations</h2>

						```scala
						// How many lines contain "Spark"?
						textFile.filter(line => line.contains("Spark")).count() 
						res3: Long = 15
						```
					</script>
					</section>
				</section>

				
<section>
				<section data-markdown>
					<script type="text/template">
						## More RDD Operations
						Finding the longest line in a text file:
						```
						textFile.map(line => line.split(" ").size)
							.reduce((a, b) => if (a > b) a else b)
res4: Long = 15
						```

						```
						scala> import java.lang.Math
						import java.lang.Math

						textFile.map(line => line.split(" ").size)
							.reduce((a, b) => Math.max(a, b))
						res5: Int = 15
						```
							
Note: This first maps a line to an integer value, creating a new RDD. reduce is called on that RDD to find the largest line count. The arguments to map and reduce are Scala function literals (closures), and can use any language feature or Scala/Java library.
					</script>
					
				</section>
				<section data-markdown>
					<script type="text/template">
						## Map Reduce
						Finding word count
						```
scala> val wordCounts = textFile.flatMap(line => line.split(" "))
 		.map(word => (word, 1)).reduceByKey((a, b) => a + b)
wordCounts: spark.RDD[(String, Int)] = spark.ShuffledAggregatedRDD@71f027b8
						```

						```
						scala> wordCounts.collect()
res6: Array[(String, Int)] = Array((means,1), (under,2),
 (this,3), (Because,1), (Python,2), (agree,1), (cluster.,1), ...)
```

Note: Here, we combined the flatMap, map and reduceByKey transformations to compute the per-word counts in the file as an RDD of (String, Int) pairs. To collect the word counts in our shell, we can use the collect action
					</script>
					<aside class="notes">
					
					</aside>
				</section>
</section>
	
				

				<section>
					<section>
						<h2>Caching</h2>
						<p class="fragment">Cluster-wide in-memory cache</p>
						<p><span class="fragment">Useful for when data is accessed repeatedly (i.e. iterative algorithms or re-querying a small dataset)</p>

						<aside class="notes">
							The interesting part is that these same functions can be used on very large data sets, even when they are striped across tens or hundreds of nodes. You can also do this interactively by connecting bin/spark-shell to a cluster
						</aside>
					</section>
					<section>
						<h2>Caching Example</h2>
						<pre><code>scala> linesWithSpark.cache()
res7: spark.RDD[String] = spark.FilteredRDD@17e51082

scala> linesWithSpark.count()
res8: Long = 15</code></pre>
					<aside class="notes">
							
					</aside>
					</section>
				</section>


				<section>
					<h2>Resources</h2>
					<ul>
						<li><a href="https://spark.apache.org/docs/latest/index.html">Spark Quick Start / Spark Docs</a></li>
						<li><a href="http://stanford.edu/~rezab/sparkclass/slides/itas_workshop.pdf">Intro To Apache Spark - Stanford</a></li>
					</ul>
				</section>
			</div>
		</div>







		<script src="lib/js/head.min.js"></script>
		<script src="js/reveal.js"></script>

		<script>

			// Full list of configuration options available at:
			// https://github.com/hakimel/reveal.js#configuration
			Reveal.initialize({
				controls: true,
				progress: true,
				history: true,
				center: true,

				transition: 'slide', // none/fade/slide/convex/concave/zoom

				// Optional reveal.js plugins
				dependencies: [
					{ src: 'lib/js/classList.js', condition: function() { return !document.body.classList; } },
					{ src: 'plugin/markdown/marked.js', condition: function() { return !!document.querySelector( '[data-markdown]' ); } },
					{ src: 'plugin/markdown/markdown.js', condition: function() { return !!document.querySelector( '[data-markdown]' ); } },
					{ src: 'plugin/highlight/highlight.js', async: true, condition: function() { return !!document.querySelector( 'pre code' ); }, callback: function() { hljs.initHighlightingOnLoad(); } },
					{ src: 'plugin/zoom-js/zoom.js', async: true },
					{ src: 'plugin/notes/notes.js', async: true }
				]
			});

		</script>

	</body>
</html>
