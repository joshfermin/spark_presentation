<!doctype html>
<html lang="en">

	<head>
		<meta charset="utf-8">

		<title>Apache Spark - Josh Fermin</title>

		<meta name="description" content="Apache Spark - Cluster Computing">
		<meta name="author" content="Josh Fermin">

		<meta name="apple-mobile-web-app-capable" content="yes" />
		<meta name="apple-mobile-web-app-status-bar-style" content="black-translucent" />

		<meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=no, minimal-ui">

		<link rel="stylesheet" href="css/reveal.css">
		<link rel="stylesheet" href="css/theme/sky.css" id="theme">

		<!-- Code syntax highlighting -->
		<link rel="stylesheet" href="lib/css/zenburn.css">

		<!-- Printing and PDF exports -->
		<script>
			var link = document.createElement( 'link' );
			link.rel = 'stylesheet';
			link.type = 'text/css';
			link.href = window.location.search.match( /print-pdf/gi ) ? 'css/print/pdf.css' : 'css/print/paper.css';
			document.getElementsByTagName( 'head' )[0].appendChild( link );
		</script>

		<!--[if lt IE 9]>
		<script src="lib/js/html5shiv.js"></script>
		<![endif]-->
	</head>

	<body>

		<div class="reveal">

			<!-- Any section element inside of this container is displayed as a slide -->
			<div class="slides">
				<section>
					<h1>Apache Spark</h1>
					<p>
						<small>By <a href="http://joshfermin.me">Josh Fermin</a></small>
					</p>
				</section>

				<section>
					<h2>What is Apache Spark?</h2>
					<p class="fragment">
						Open source data analytics cluster computing framework
					</p>
					<p class="fragment">
						Real-Time vs Interactive vs Batch processing
					</p>
					<p class="fragment">
						Write apps in Java, Scala or Python.
					</p>
					<p class="fragment">
						Map Reduce is only a part of supported capabilities.
					</p>

					<aside class="notes">
						Versatile data-processing framework – can run both in memory and on disk

						Allows you to do a variety of things including real-time vs interactive vs batch

						can write applications in Java, Scala, Python.

						fast interactive queries, streaming analytics, graph analytics and machine learning.
					</aside>
				</section>

				<!-- Example of nested vertical slides -->
				<section>
					<section>
						<h2>History</h2>
						<img src="lib/img/timeline.png"></img>
						<aside class="notes">
							Spark was developed in 2009 at UC Berkeley AMPLab, open sourced in 2010, and became a top-level Apache project in February, 2014. It has since become  one of the largest open source communities in big data, with over 200 contributors in 50+ organizations.
						</aside>
					</section>
					<section>
						<h2>History - Map Reduce</h2>
						<p>
							Two major problems:
						</p>
						<ul>
							<li class="fragment">difficulty programming in Map Reduce</li>
							<li class="fragment">Batch style jobs not fitting all use cases</li>
						</ul>
						<aside class="notes">
						Writing MapReduce code sucks, because the API is rudimentary, hard to test, and easy to screw up. Tool like Cascading, Pig, Hive, etc., make this easier, but that’s just more evidence that the core API is fundamentally flawed.

						It requires lots of code to perform even the simplest of tasks.

						Everything gets written to disk, including all the interim steps.

						In many cases you need a chain of jobs to perform your analysis, making #1 even worse.

						
						</aside>
					</section>
					<section>
						<h2>History - Workarounds</h2>
						<img src="lib/img/workarounds.png"></img>

						<aside class="notes">
							many different technologies came out because of the limitations of map reduce. Storm being a real time processing framework. Pregel - large scale graph computing
						</aside>
					</section>
				</section>

				<section>
					<section>
						<h2>Spark - Goals</h2>
						<p class="fragment">Generalize Map Reduce</p>
						<ul>
							<li class="fragment">fast data sharing</li>
							<li class="fragment">general DAGs (Directed Acyclic Graphs)</li>
						</ul>
						<aside class="notes">
							goal was to generalize map reduce to support new apps with the same engine - SPARK. Two small additions are enough to express the previous models.
							approach that is more efficient for engine and much simpler for end users
						</aside>
					</section>
					<section>
						<img src="lib/img/codesize.png"></img>
					</section>
				</section>

				<section>
					<section>
						<h2>Learning Spark API</h2>
						<p>
						<a href="https://spark.apache.org/docs/latest/quick-start.html">Problem:</a> Count how many lines a word shows up in a document
						</p>
						<aside class="notes">
							
						</aside>
					</section>
					<section>
						<h2>Spark Shell - Scala</h2>
						<small>Spark Download <a href="https://spark.apache.org/downloads.html">here</a> (use prebuilt version)</small>
							<pre><code>./bin/spark-shell/</pre></code>
							<pre><code>./bin/pyspark/</pre></code>
							
						<aside class="notes">
							Starts the shell in scala using ./bin/spark-shell/
							can also start in python ./bin/pyspark

							can download spark there, need to build or have prebuilt version for these commands to work
						</aside>
					</section>

					<section>
					<h2> Spark Context </h2>
					<pre><code>scala> val textFile = sc.textFile("README.md")
textFile: spark.RDD[String] = spark.MappedRDD@2ee9b6e3</pre></code>
					<ul>
						<li>Start spark context</li>
						<li>Creates RDD (Resilient Distributed Dataset)</li>
					</ul>
					<aside class="notes">
						Next line starts a spark context which represents a connection to a spark cluster.
						Can be used to create RDDs.
					</aside>
					</section>
					
					<section>
						<h2>RDDs</h2>
						<ul>
							<li>Spark's primary abstraction</li>
							<li>Distributed collection of items</li>
							<li>Can be stored in the volatile memory or in a persistent storage </li>
						</ul>
					</section>

					<section>
						<h2>RDDs</h2>
						<img src="http://3.bp.blogspot.com/-J7R0TMXgz04/UuKIuoeANDI/AAAAAAAAKxM/RyuhMwSLLS0/s1600/mr-flow.png"></img>
						<ul>
						<li> Reading and writing happens too often for each Map Reduce (MR)</li>
						</ul>
					</section>

					<section>
						<h2>RDDs</h2>
						<img src="http://2.bp.blogspot.com/-QjW6MrkQ4hE/UuKIgacmG4I/AAAAAAAAKxE/EGXeX7UUDxc/s1600/rdd-flow.png"></img>
						<ul>
						<li> Transformations from one to another are through memory and doesn't touch the disk (except for RDD2)</li>
						<li>When the memory runs out, it is usually moved over to persistent storage.</li>
						</ul>
						<aside class="notes">Note that in the above flow, the RDD2 is persisted to disk because of Check Pointing. In the work flow, for any failure during the transformations t3 or t4, the entire work flow need not be played back because the RDD2 is persisted to disk. It would be enough if transformation t3 and t4 are played back.</aside>
					</section>

					<section data-markdown>
					<script type="text/template">
					<h2>RDD Actions</h2>
					```scala
					scala> textFile.count() // Number of items in this RDD
					res0: Long = 126

					scala> textFile.first() // First item in this RDD
					res1: String = # Apache Spark
					```

					Note: RDDs have actions which return values. 
					</script>
					</section>

					<section>
					<h2>RDD Transformations</h2>
					<pre><code>
val linesWithSpark = textFile.filter(line => line.contains("Spark"))
linesWithSpark: spark.RDD[String] = spark.FilteredRDD@7dd4af09
					</code></pre>
					<aside class="notes">
					RDDs also have transformations which returns a pointer to a new RDDs, in this case it will return a subset of the previous RDD with the lines that only contain the word "Spark"
					</aside>
					</section>

					<section data-markdown>
					<script type="text/template">
						<h2>Chaining Actions and transformations</h2>

						```scala
						// How many lines contain "Spark"?
						textFile.filter(line => line.contains("Spark")).count() 
						res3: Long = 15
						```
					</script>
					</section>
				</section>

				
<section>
				<section data-markdown>
					<script type="text/template">
						## More RDD Operations
						Finding the longest line in a text file:
						```
						textFile.map(line => line.split(" ").size)
							.reduce((a, b) => if (a > b) a else b)
res4: Long = 15
						```

						```
						scala> import java.lang.Math
						import java.lang.Math

						textFile.map(line => line.split(" ").size)
							.reduce((a, b) => Math.max(a, b))
						res5: Int = 15
						```
							
Note: This first maps a line to an integer value, creating a new RDD. reduce is called on that RDD to find the largest line count. The arguments to map and reduce are Scala function literals (closures), and can use any language feature or Scala/Java library.
					</script>
					
				</section>
				<section data-markdown>
					<script type="text/template">
						## Map Reduce
						Finding word count
						```
scala> val wordCounts = textFile.flatMap(line => line.split(" "))
 		.map(word => (word, 1)).reduceByKey((a, b) => a + b)
wordCounts: spark.RDD[(String, Int)] = spark.ShuffledAggregatedRDD@71f027b8
						```

						```
						scala> wordCounts.collect()
res6: Array[(String, Int)] = Array((means,1), (under,2),
 (this,3), (Because,1), (Python,2), (agree,1), (cluster.,1), ...)
```

Note: Here, we combined the flatMap, map and reduceByKey transformations to compute the per-word counts in the file as an RDD of (String, Int) pairs. To collect the word counts in our shell, we can use the collect action
					</script>
					<aside class="notes">
					
					</aside>
				</section>
</section>
	
				

				<section>
					<section>
						<h2>Caching</h2>
						<p class="fragment">Cluster-wide in-memory cache</p>
						<p><span class="fragment">Useful for when data is accessed repeatedly (i.e. iterative algorithms or re-querying a small dataset)</p>

						<aside class="notes">
							The interesting part is that these same functions can be used on very large data sets, even when they are striped across tens or hundreds of nodes. You can also do this interactively by connecting bin/spark-shell to a cluster
						</aside>
					</section>
					<section>
						<h2>Caching Example</h2>
						<pre><code>scala> linesWithSpark.cache()
res7: spark.RDD[String] = spark.FilteredRDD@17e51082

scala> linesWithSpark.count()
res8: Long = 15</code></pre>
					<aside class="notes">
							
					</aside>
					</section>
				</section>

				<section>
				<h2>Where to go from here</h2>
				<ul>
				<li><a href="https://spark.apache.org/docs/latest/programming-guide.html">Spark Programming Gudie</a></li>
				<li><a href="https://spark.apache.org/docs/latest/cluster-overview.html">Deployment overview</a></li>
				<li>Spark Examples</li>
				</ul>
				</section>

				<section>
				<h2>Who uses spark?</h2>
				<ul>
				<li>SK Telecom - Analyze mobile usage patterns</li>
				<li>Freeman Lab - analyzing/visualizing patterns in large scale recordings of brain acitivty</li>
				<li>Yandex - Using spark to process islands identified from  a search robot</li>
				</ul>
				</section>
				<section>
					<h2>Resources</h2>
					<ul>
						<li><a href="https://spark.apache.org/docs/latest/index.html">Spark Quick Start / Spark Docs</a></li>
						<li><a href="http://stanford.edu/~rezab/sparkclass/slides/itas_workshop.pdf">Intro To Apache Spark - Stanford</a></li>
						<li><a href="http://www.thecloudavenue.com/2014/01/resilient-distributed-datasets-rdd.html">Learning about RDDs</a></li>
					</ul>
				</section>
			</div>
		</div>







		<script src="lib/js/head.min.js"></script>
		<script src="js/reveal.js"></script>

		<script>

			// Full list of configuration options available at:
			// https://github.com/hakimel/reveal.js#configuration
			Reveal.initialize({
				controls: true,
				progress: true,
				history: true,
				center: true,

				transition: 'slide', // none/fade/slide/convex/concave/zoom

				// Optional reveal.js plugins
				dependencies: [
					{ src: 'lib/js/classList.js', condition: function() { return !document.body.classList; } },
					{ src: 'plugin/markdown/marked.js', condition: function() { return !!document.querySelector( '[data-markdown]' ); } },
					{ src: 'plugin/markdown/markdown.js', condition: function() { return !!document.querySelector( '[data-markdown]' ); } },
					{ src: 'plugin/highlight/highlight.js', async: true, condition: function() { return !!document.querySelector( 'pre code' ); }, callback: function() { hljs.initHighlightingOnLoad(); } },
					{ src: 'plugin/zoom-js/zoom.js', async: true },
					{ src: 'plugin/notes/notes.js', async: true }
				]
			});

		</script>

	</body>
</html>
